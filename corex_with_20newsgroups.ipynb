{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import corex as cx\n",
    "import vis_corex as vcx\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from os.path import basename\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 20Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = None\n",
    "remove = (\n",
    "    'headers',\n",
    "    'footers'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "all_data = data_train.data + data_test.data\n",
    "all_names = [basename(f) for f in data_train.filenames] + [basename(f) for f in data_test.filenames]\n",
    "all_labels = list(enumerate(list(data_train.target) + list(data_test.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binarize = True\n",
    "max_vocab = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_unigrams = CountVectorizer(\n",
    "    binary=binarize,\n",
    "    max_features=max_vocab\n",
    ")\n",
    "\n",
    "vectorized_data = vectorized_unigrams.fit_transform(all_data).toarray()\n",
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build single layer `CoRex` representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 20      # number of Y_s; number of clusters?  m?\n",
    "cluster_dim = 2      # dimension of each hidden; k?\n",
    "max_samples = 10000\n",
    "max_iter=50          # 20 iterations didn't come close to convergence\n",
    "random_seed = 1978"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corex, rep size: 20 2\n",
      "Marginal description:  discrete\n"
     ]
    }
   ],
   "source": [
    "corex_layer_1 = cx.Corex(\n",
    "    n_hidden=num_hidden,            \n",
    "    dim_hidden=cluster_dim,            \n",
    "    marginal_description='discrete',   # for discrete data\n",
    "    max_iter=max_iter,\n",
    "    max_samples=max_samples,\n",
    "    seed=random_seed,\n",
    "    verbose=True,\n",
    "    n_cpu=2,\n",
    "    ram=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.001  0.071  0.03   0.001  0.025  0.063  0.062 -0.001  0.003  0.017  0.01   0.318  0.013  0.012  0.001  0.506 -0.     0.105  0.003  0.117]\n",
      "[  0.001   0.189   0.457   0.024  -0.     -0.005   0.144   0.001   0.023   0.051   0.011   0.661   0.036   0.017   0.003  21.08    0.001   0.197   0.014  -0.004]\n",
      "[  0.002   0.03   -0.009   0.168   0.016   0.042   0.192   0.031  -0.005  -0.01    0.057  -0.004   0.001   0.084   0.016   0.039   0.003   0.119   0.029  13.693]\n",
      "[  0.016   0.045  13.548   0.549   0.002   0.017   0.209  -0.004  -0.      0.059  -0.002   0.078  -0.004   0.206  -0.001   0.338   0.002   0.05    0.018  -0.066]\n",
      "[  0.111   0.018  -0.035   0.563   0.003   0.027   0.216   0.003   0.012   0.048   0.046   0.071   0.015   0.238   0.054   0.114   0.005   0.01    0.012  18.004]\n",
      "[  0.359   0.014  14.741   0.56    0.004   0.032   0.213   0.007   0.      0.079   0.001   0.017   0.067   0.244   0.      0.294   0.008   0.012   0.005   0.289]\n",
      "[  0.492   0.062   0.13    0.559   0.003   0.043   0.246   0.017   0.015   0.1     0.198   0.042   0.135   0.243   0.314   1.713   0.016   0.014   0.101  24.447]\n",
      "[  0.508   0.351  24.855   0.578   0.016   0.078   0.436   0.059   0.043   0.158   0.006   0.179   0.325   0.255   0.021   0.286   0.025   0.05    0.052  -0.056]\n",
      "[  0.513   0.027   0.119   0.566   0.009   0.046   0.238   0.062   0.001   0.123   0.104   0.059   0.146   0.239   0.054   0.194   0.031   0.023   0.014  20.938]\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "ys_layer_1 = corex_layer_1.fit_transform(vectorized_data)\n",
    "print(\"time to compute: {} seconds\".format(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.labels` is the size of your data.  And `.labels[i]` gives you the values for each $Y_j$ for the $i^{th}$ datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corex_layer_1.labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So....in theory, two documents with the same `label`, should also have similar `label vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_grouped = groupby(all_labels, lambda x: x[1])\n",
    "labels_dict = {}\n",
    "for idx, group in labels_grouped:\n",
    "    just_docs = list(map(lambda x: x[0], group))\n",
    "    if idx in labels_dict:\n",
    "        labels_dict[idx].extend(just_docs)\n",
    "    else:\n",
    "        labels_dict[idx] = just_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_doc_label_1, same_doc_label_2 = labels_dict[0][:2]\n",
    "print(\"two docs with label=0: {}, {}\".format(same_doc_label_1, same_doc_label_2))\n",
    "corex_layer_1.labels[same_doc_label_1], corex_layer_1.labels[same_doc_label_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And presumably two docs from very different gropus should have dissimilar `label vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(data_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_doc_label_1 = labels_dict[0][0]\n",
    "diff_doc_label_2 = labels_dict[12][0]\n",
    "corex_layer_1.labels[diff_doc_label_1], corex_layer_1.labels[diff_doc_label_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so convincing...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will identify which cluster each word should belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = vectorized_unigrams.vocabulary_\n",
    "i2w = dict((i,w) for w,i in w2i.items())\n",
    "i2w[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = corex_layer_1.clusters\n",
    "clusters_grouped = groupby(enumerate(clusters), lambda x: x[1])\n",
    "clusters_dict = {}\n",
    "for idx, group in clusters_grouped:\n",
    "    just_words = [i2w[i] for i in map(lambda x: x[0], group)]\n",
    "    if idx in clusters_dict:\n",
    "        clusters_dict[idx].extend(just_words)\n",
    "    else:\n",
    "        clusters_dict[idx] = just_words\n",
    "for c, words in clusters_dict.items():\n",
    "    print(\"cluster {} has {} words\".format(c, len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not at all balanced...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcx.vis_rep(\n",
    "    corex=corex_layer_1, \n",
    "    data=vectorized_data,\n",
    "    row_label=all_names,\n",
    "    column_label=None,\n",
    "    prefix=\"20newsgroups_viz\",\n",
    "    topk=num_hidden\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
