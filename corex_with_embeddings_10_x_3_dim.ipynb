{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import corex as cx\n",
    "import vis_corex as vcx\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "from time import time\n",
    "import re\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings using `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', '.', ',', 'is', 'and']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_gensim = models.KeyedVectors.load_word2vec_format(fname=\"data/vectors_Goldberg_sample.txt\", binary=False)\n",
    "# capture indexes\n",
    "word_idxs = list(map(lambda x: x[0], embeddings_gensim.vocab.items()))\n",
    "word_idxs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings into `numpy` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix = embeddings_gensim.syn0\n",
    "emb_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build single layer `CoRex` representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 10      # number of Y_s; number of clusters?  m?\n",
    "cluster_dim = 3      # dimension of each hidden; k?\n",
    "max_samples = emb_matrix.shape[0]\n",
    "max_iter=12          # previous runs show this is sufficient\n",
    "random_seed = 1978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corex, rep size: 10 3\n",
      "Marginal description:  gaussian\n"
     ]
    }
   ],
   "source": [
    "corex_layer_1 = cx.Corex(\n",
    "    n_hidden=num_hidden,            \n",
    "    dim_hidden=cluster_dim,            \n",
    "    marginal_description='gaussian',   # for continuous data\n",
    "    max_iter=max_iter,\n",
    "    max_samples=max_samples,\n",
    "    seed=random_seed,\n",
    "    verbose=True,\n",
    "    ram=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.002  0.    -0.    -0.     0.    -0.     0.002 -0.001 -0.    -0.   ]\n",
      "[ 0.001  0.108  0.026  0.032  0.044  0.007  0.058  0.016  0.026  0.025]\n",
      "[ 0.004  1.164  0.014  0.39   2.054  0.022  0.129  0.002  0.011  0.218]\n",
      "[ 0.109  0.819  0.015  1.812  2.642  0.107  0.13   0.003  0.037  1.586]\n",
      "[ 2.82   0.716  0.192 -0.135  0.675  0.533  0.29   0.029  0.184  0.788]\n",
      "[ 0.883  0.748  1.032  1.462  1.977  0.583  0.736  0.151  0.514  0.901]\n",
      "[ 3.526  1.29   1.005  0.533  1.71   0.47   0.516  0.38   0.527  0.495]\n",
      "[ 2.407  1.884  1.103  0.75   2.49   0.566  0.73   0.356  0.568  0.778]\n",
      "[ 2.016  2.281  1.219  0.87   2.672  0.576  0.831  0.302  0.556  0.8  ]\n",
      "[ 1.928  2.03   1.271  0.819  3.228  0.663  0.828  0.33   0.594  0.623]\n",
      "[ 1.655  1.907  1.133  0.798  3.28   0.983  0.85   0.358  0.698  0.373]\n",
      "[ 1.145  1.796  1.26   0.769  3.718  1.06   0.843  0.378  0.765  0.173]\n",
      "Overall tc: 11.9085004576\n",
      "Best tc: 11.9085004576\n",
      "time to compute: 193.36731004714966 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "ys_layer_1 = corex_layer_1.fit_transform(emb_matrix)\n",
    "print(\"time to compute: {} seconds\".format(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.labels` is the size of your data.  And `.labels[i]` gives you the values for each $Y_j$ for the $i^{th}$ datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 0, 0, 0, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corex_layer_1.labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So....in theory, two words with a high `cosine similarity` in embedding space, should also have similar `label vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('their', 0.8471860885620117),\n",
       " ('its', 0.7965162396430969),\n",
       " ('my', 0.7934165000915527),\n",
       " ('your', 0.7914167642593384),\n",
       " ('our', 0.7593696713447571),\n",
       " ('whose', 0.738511323928833),\n",
       " ('her', 0.6965184211730957),\n",
       " ('the', 0.4893943667411804),\n",
       " ('``', 0.4586181342601776),\n",
       " (\"'ll\", 0.4517585039138794)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_gensim.most_similar([\"his\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 2, 2, 0, 0, 0, 0, 1, 1, 1]), array([0, 1, 2, 0, 0, 0, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_of_his = word_idxs.index('his')\n",
    "index_of_their = word_idxs.index('their')\n",
    "\n",
    "corex_layer_1.labels[index_of_his], corex_layer_1.labels[index_of_their]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And presumably two `dissimilar` words in embedding space should have dissimilar `label vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eleven', 0.07403583079576492),\n",
       " ('announced', 0.06627192348241806),\n",
       " ('premiere', 0.05658423900604248)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_gensim.most_similar([\"his\"], topn=len(word_idxs))[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 2, 2, 0, 0, 0, 0, 1, 1, 1]), array([1, 0, 2, 1, 2, 1, 1, 2, 1, 0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_of_premiere = word_idxs.index('premiere')\n",
    "\n",
    "corex_layer_1.labels[index_of_his], corex_layer_1.labels[index_of_premiere]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.tcs` provides the `total correlation` captured by each $Y_j$, and are sorted from greatest to least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.718,  1.796,  1.26 ,  1.145,  1.06 ,  0.843,  0.769,  0.765,  0.378,  0.173])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corex_layer_1.tcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this case, $Y_0$ captures the most `total correlation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will identify which cluster each `dimension` of the word embedding should belong to.  See this example:\n",
    "\n",
    "```\n",
    "X = np.array([[0,0,0,0,0], # A matrix with rows as samples and columns as variables.\n",
    "              [0,0,0,1,1],\n",
    "              [1,1,1,0,0],\n",
    "              [1,1,1,1,1]], dtype=int)\n",
    "\n",
    "layer1 = ce.Corex(n_hidden=2, dim_hidden=2, marginal_description='discrete', smooth_marginals=False)  \n",
    "\n",
    "layer1.fit(X)  # Fit on data. \n",
    "\n",
    "layer1.clusters  # Each variable/column is associated with one Y_j\n",
    "# array([0, 0, 0, 1, 1])\n",
    "```\n",
    "\n",
    "You can see that the first three `dimension`s belong together (in cluster `0`) and the last two together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 7, 3, 6, 5, 8, 6, 0, 2, 2, 4, 7, 0, 4, 0, 0, 6, 4, 6, 1, 5, 1, 2, 2, 7, 2, 9, 0, 6, 2, 5, 0, 3, 2, 0, 7, 2, 2, 5, 7, 4, 0, 2, 8, 1, 0, 1, 0, 7, 8, 3, 5, 1, 0, 6, 3, 5, 8, 9, 6, 4, 0, 1,\n",
       "       0, 2, 0, 2, 1, 4, 8, 1, 1, 0, 3, 3, 5, 3, 4, 0, 1, 5, 0, 1, 0, 0, 5, 5, 1, 9, 3, 0, 0, 2, 0, 7, 5, 0, 2, 1, 5, 0, 7, 4, 0, 1, 0, 0, 0, 1, 0, 6, 9, 0, 0, 3, 0, 4, 0, 0, 6, 1, 6, 8, 0, 0, 7, 7,\n",
       "       6, 8, 5, 3, 1, 6, 1, 3, 7, 2, 7, 2, 1, 0, 5, 1, 8, 2, 0, 0, 1, 7, 2, 5, 1, 9, 1, 1, 4, 0, 2, 2, 7, 0, 3, 7, 4, 8, 2, 8, 4, 1, 1, 3, 0, 0, 0, 0, 3, 7, 2, 1, 6, 3, 0, 7, 0, 6, 5, 0, 2, 2, 8, 1,\n",
       "       8, 0, 5, 0, 8, 2, 5, 1, 0, 4, 1, 2, 1, 6, 6, 1, 4, 5, 4, 0, 0, 0, 1, 0, 7, 7, 6, 5, 0, 0, 6, 7, 0, 2, 1, 1, 0, 9, 0, 2, 2, 6, 2, 1, 0, 6, 0, 0, 1, 3, 0, 0, 1, 5, 1, 1, 3, 2, 6, 1, 0, 8, 0, 5,\n",
       "       5, 4, 5, 7, 4, 0, 5, 1, 7, 9, 1, 1, 6, 8, 7, 7, 5, 6, 8, 4, 4, 2, 2, 3, 3, 5, 4, 6, 8, 2, 3, 1, 0, 3, 6, 1, 7, 3, 3, 1, 0, 7, 4, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = corex_layer_1.clusters\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a clustering of each `dimension` of the word embeddings.  Not sure what meaning this may carry...if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 4 has 23 dims\n",
      "cluster 6 has 26 dims\n",
      "cluster 7 has 26 dims\n",
      "cluster 3 has 23 dims\n",
      "cluster 5 has 27 dims\n",
      "cluster 8 has 17 dims\n",
      "cluster 0 has 70 dims\n",
      "cluster 2 has 34 dims\n",
      "cluster 1 has 47 dims\n",
      "cluster 9 has 7 dims\n"
     ]
    }
   ],
   "source": [
    "clusters = corex_layer_1.clusters\n",
    "clusters_grouped = groupby(enumerate(clusters), lambda x: x[1])\n",
    "clusters_dict = {}\n",
    "for idx, group in clusters_grouped:\n",
    "    just_dims = list(map(lambda x: x[0], group))\n",
    "    if idx in clusters_dict:\n",
    "        clusters_dict[idx].extend(just_dims)\n",
    "    else:\n",
    "        clusters_dict[idx] = just_dims\n",
    "for c, dims in clusters_dict.items():\n",
    "    dims_total += len(dims)\n",
    "    print(\"cluster {} has {} dims\".format(c, len(dims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs code to generate a bunch of visualizations, which will end up in the directory called `embedding_viz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vcx.vis_rep(\n",
    "#     corex=corex_layer_1, \n",
    "#     data=emb_matrix,\n",
    "#     row_label=word_idxs,\n",
    "#     column_label=None,\n",
    "#     prefix=\"embedding_viz\",\n",
    "#     topk=num_hidden\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_groups = \"embedding_viz_10_x_3_dim/text_files/groups_no_overlaps.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_regex = r'Group num: ([0-9]+), TC.*'\n",
    "groups = {}\n",
    "groups_list = []\n",
    "with open(path_to_groups, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"Group num\"):\n",
    "            group_number = re.match(group_regex, line)[1]\n",
    "            if groups_list:\n",
    "                groups[int(group_number) - 1] = groups_list\n",
    "            groups_list = []\n",
    "        else:\n",
    "            dim, val = line.rstrip().split(\",\")\n",
    "            groups_list.append((dim, val))\n",
    "groups[int(group_number)] = groups_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 102 dims in group 0 with a total TCS of 3.7178747457547745:\n",
      "296,84,280,238,213,125,184,161,15,85,195,118,32,288,16,72,239,267,47,140,110,8,220,211,147,236,92,136,155,241,42,175,48,64,279,157,263,121,244,28,128,188,146,224,3,36,79,108,289,174,135,182,56,294,176,113,111,75,148,141,62,221,230,200,177,193,95,226,170,74,103,206,23,81,264,63,57,291,185,243,172,178,293,114,273,160,104,124,255,67,53,94,77,162,101,290,133,145,233,165,286,19\n",
      "----\n",
      "There are 41 dims in group 1 with a total TCS of 1.795976832579594:\n",
      "35,173,242,143,73,227,266,109,191,11,54,207,187,202,82,66,132,275,276,204,45,251,71,295,107,20,68,256,190,13,168,152,214,80,179,212,228,246,287,199,123\n",
      "----\n",
      "There are 37 dims in group 2 with a total TCS of 1.2604112385829591:\n",
      "78,197,262,156,14,89,225,265,69,117,38,154,59,163,261,247,252,201,210,91,116,22,43,169,98,138,61,52,285,159,122,298,112,258,192,27,55\n",
      "----\n",
      "There are 29 dims in group 3 with a total TCS of 1.145412547302473:\n",
      "25,41,34,1,270,30,150,277,189,33,37,10,40,194,217,299,257,231,208,232,18,158,274,137,99,39,24,65,4\n",
      "----\n",
      "There are 26 dims in group 4 with a total TCS of 1.0596895788915077:\n",
      "0,272,248,90,88,245,281,219,209,166,44,164,260,282,105,5,87,269,198,31,9,151,297,100,130,249\n",
      "----\n",
      "There are 23 dims in group 5 with a total TCS of 0.8432872328889448:\n",
      "253,144,196,284,106,167,153,50,6,215,142,222,216,254,234,58,70,129,86,180,76,21,120\n",
      "----\n",
      "There are 11 dims in group 6 with a total TCS of 0.7690524024701383:\n",
      "237,17,186,29,218,126,60,250,283,203,83\n",
      "----\n",
      "There are 12 dims in group 7 with a total TCS of 0.7652779511339756:\n",
      "2,271,205,12,223,93,46,259,149,49,278,268\n",
      "----\n",
      "There are 9 dims in group 8 with a total TCS of 0.37808261098044665:\n",
      "119,240,134,26,127,229,235,292,139\n",
      "----\n",
      "There are 10 dims in group 9 with a total TCS of 0.1734353170168372:\n",
      "183,131,171,181,115,97,7,102,96,51\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for g, dims in groups.items():\n",
    "    print(\"There are {} dims in group {} with a total TCS of {}:\\n{}\".format(\n",
    "        len(dims), g, corex_layer_1.tcs[g], \",\".join(list(map(lambda x: x[0], dims)))\n",
    "        )\n",
    "    )\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
